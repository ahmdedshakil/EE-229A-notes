\lec{1}{08/28/25}{Intro to Information Theory, Historical background}

Information theory answers two fundamental questions: What are the fundamental limits of data compression (answer: the entropy \(H\)) and what are the fundamental limits of reliable communication (answer: the channel capacity \(C\)). Although it is usually considered a subset of communication theory, information theory has made fundamental contributions to many other fields. 

The field of information theory was founded in the 1940s by Claude Shannon. He proved that it is possible to send information at a positive rate with negligible probability of error (near zero) for all communication rates below channel capacity. He also argued that random processes have an irreducible complexity below which the signal cannot be compressed, he called this the \textit{entropy}. Shannon argued that if the entropy of the source is less than the capacity of the channel, then asymptotically error-free communication can be achieved. Although we now know the ultimate limits of communication thanks to Shannon's work, achieving these limits have not been easy or  computationally practical in most cases. 